import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score, classification_report
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')
# Define a simple classifier on top of BERT
class BERTClassifier(nn.Module):
    def __init__(self, bert_model, num_classes):
        super(BERTClassifier, self).__init__()
        self.bert = bert_model
        self.fc = nn.Linear(bert_model.config.hidden_size, num_classes)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        pooled_output = last_hidden_state[:, 0]
        logits = self.fc(pooled_output)
        return logits
data = pd.read_csv(r"C:\Users\kkbmu\OneDrive\Desktop\mini project\Code\train2.csv")
# Example data
texts = data['text'].tolist()  # Assuming 'data' is your DataFrame containing text data
labels = data['target'].tolist()  # Assuming 'target' column contains the labels
# Tokenize the texts
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
# Convert labels to tensors
labels = torch.tensor(labels)
# Create DataLoader for the dataset
dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)
loader = DataLoader(dataset, batch_size=32, shuffle=True)
# Initialize BERTClassifier
num_classes = 2  # Assuming binary classification
model = BERTClassifier(bert_model, num_classes)
# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
# Train the model
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch in loader:
        input_ids, attention_mask, labels = batch
        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
# Evaluate the model
model.eval()
all_preds = []
all_labels = []
for batch in loader:
    input_ids, attention_mask, labels = batch
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
# Calculate accuracy and print classification report
accuracy = accuracy_score(all_labels, all_preds)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(all_labels, all_preds))
# Save the trained BERT model
path_to_model = r"C:\Users\kkbmu\OneDrive\Desktop\mini project\Code\model.pth"
torch.save(model.state_dict(), path_to_model)
